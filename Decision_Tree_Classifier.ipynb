{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision Tree Classifier.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO8w/nxz2ucN89DAu+kax5Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArpitaChatterjee/Decision-Tree-Classifier/blob/main/Decision_Tree_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8T_w0E3Iinm",
        "outputId": "bef1814b-1979-402f-ebbc-d3979ebae4b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elrvQF8TIEcG"
      },
      "source": [
        "#set my sample dataset\n",
        "train_data=[\n",
        "    ['green', 3, 'mango'],\n",
        "    ['yellow', 3, 'mango'],\n",
        "    ['red', 1, 'grape'],\n",
        "    ['red', 1, 'grape'],\n",
        "    ['yellow', 3, 'lemon'],\n",
        "]\n",
        "#col labels to print tree\n",
        "header=['color', 'diameter', 'label']\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p27-nnQ2I0hD"
      },
      "source": [
        "\n",
        "def unique_val(rows, col):\n",
        "    #find unique values for col in dataset\n",
        "    return set([row[col] for row in rows])\n",
        "##\n",
        "#demo:\n",
        "#unique_val(train_data, 0)..\n",
        "def class_counts(rows):\n",
        "    \"\"\"counts no of eaxch type in dataset\"\"\"\n",
        "    counts={}#dictionary of labels->count\n",
        "    for row in rows:\n",
        "        #in dataset labels is always the last col\n",
        "        label=row[-1]\n",
        "        if label not in counts:\n",
        "            counts[label]=0\n",
        "        counts[label]+=1\n",
        "    return counts\n",
        "\n",
        "#demo:\n",
        "#class_count(train_data)\n",
        "\n",
        "def is_numeric(value):\n",
        "    \"\"\"test if value is numeric or not\"\"\"\n",
        "    return isinstance(value, int) or isinstance(value, float)\n",
        "#demo:\n",
        "#is_numeric(7)\n",
        "#isnumeric('red')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLd-tFvCI6mU"
      },
      "source": [
        "class question:\n",
        "    \"\"\"a q. is usd to partition a dataset\"\"\"\n",
        "    \n",
        "    def __init__(self, column ,value):\n",
        "        self.column=column\n",
        "        self.value=value\n",
        "    \n",
        "    def match(self, example):\n",
        "        #compare featur val in eq with those in q.\n",
        "        val= example[self.column]\n",
        "        if is_numeric(val):\n",
        "            return val>= self.value\n",
        "        else:\n",
        "            return val== self.value\n",
        "        \n",
        "    def __repr__(self):\n",
        "        #mthd to print the q. in readable mthd\n",
        "        condition=\"==\"\n",
        "        if is_numeric(self.value):\n",
        "            condition=\">=\"\n",
        "        return \"is %s %s %s?\" %(\n",
        "        header[self.column], condition, str(self.value))\n",
        "\n",
        "\n",
        "def partition(rows, question):\n",
        "    '''dataset for each row in the dataset to check if it matches the q. if so \n",
        "    add it to the true rows otherwise false rows'''\n",
        "    \n",
        "    true_rows, false_rows= [] , []\n",
        "    for row in rows:\n",
        "        if question.match(row):\n",
        "            true_rows.append(row)\n",
        "        else:\n",
        "            false_rows.append(row)\n",
        "    return true_rows, false_rows\n",
        "\n",
        "##partition traindatast if rows r red or not\n",
        "#true_rows, false_rows = partition(training_data, question(0, 'red'))\n",
        "#true_row==contain all \"red\" rows\n",
        "#false_rows == cpntain everything else\n",
        "\n",
        "def gini(rows):\n",
        "    '''calculate gini impurity for a list of rows'''\n",
        "    counts=class_counts(rows)\n",
        "    impurity =1\n",
        "    for lbl in counts:\n",
        "        prob_of_lbl = counts[lbl]/float(len(rows))\n",
        "        impurity -=prob_of_lbl**2\n",
        "    return impurity\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urOkMe3VJEql"
      },
      "source": [
        "\n",
        "def info_gain(left, right, curr_uncertainity):\n",
        "    '''Information Gain== the uncertainity of the starting node- wt. impurity of 2 child node'''\n",
        "    \n",
        "    p= float(len(left))/ (len(left)+len(right))\n",
        "    return curr_uncertainity-p*gini(left)-(1-p)*gini(right)\n",
        "\n",
        "##calculate uncertainity of our training dataset\n",
        "#curr_uncertainity = gini(training_data)\n",
        "\n",
        "#ig when partition is on 'green'\n",
        "#true_rows, false_rows = partition(training_data, question(0, 'green'))\n",
        "#ingo_gain(true_rows, false_rows, curr_uncertainity)\n",
        "\n",
        "#ig when partition is on 'red'\n",
        "##true_rows, false_rows = partition(training_data, question(0, 'red'))\n",
        "#ingo_gain(true_rows, false_rows, curr_uncertainity)\n",
        "\n",
        "\n",
        "def find_best_split(rows):\n",
        "    '''find the best q. to ask by iterationg over every feature / value and calcualting the ig'''\n",
        "    best_gain= 0 #to keep track of best ig\n",
        "    best_question = None #keep train of the feature value that produced it\n",
        "    curr_uncertainity = gini(rows)\n",
        "    n_features = len(rows[0])-1 #no of col\n",
        "    for col in range(n_features):\n",
        "        values = set([row[col] for row in rows]) #unique values in col\n",
        "        for val in values:#foreach val\n",
        "            ques = question(col, val) \n",
        "            \n",
        "            #try spliting the dataset\n",
        "            true_rows, false_rows = partition(rows, question)\n",
        "            \n",
        "            #skip the split if it doesnt divide the dataset\n",
        "            \n",
        "            if len(true_rows) == 0 or len(false_rows) == 0:\n",
        "                continue\n",
        "                \n",
        "            #ig from this split\n",
        "            gain= info_gain(true_rows, false_rows, curr_uncertainity)\n",
        "            \n",
        "            #use '>' in stead of '>=' in the below code\n",
        "            if gain >= best_gain:\n",
        "                best_gain, best_question= gain, ques\n",
        "    return best_gain, best_question\n",
        "\n",
        "#demo:\n",
        "#to find the best q. to ask 1st for our toydataset\n",
        "#best_gain, best_question = first_best_split(training_data)\n",
        "#fyi: is color == 'red' is just as good\n",
        "\n",
        "class leaf:\n",
        "    '''a leaf node classifies data'''\n",
        "    def __init__(self, rows):\n",
        "        self.prediction = class_counts(rows)\n",
        "        \n",
        "        \n",
        "class decision_node:\n",
        "    '''a decision node asks a q. --which holds reference to the q., and to the 2 child nodes'''\n",
        "    \n",
        "    def __init__(self, ques, true_branch, false_branch):\n",
        "        self.ques= ques\n",
        "        self.true_branch = true_branch\n",
        "        self.false_branch = false_branch\n",
        "        "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FmUA3OCJLVd"
      },
      "source": [
        "\n",
        "def build_tree(rows):\n",
        "    '''build the tree'''\n",
        "    \n",
        "    #ty partition the datset on ech unique attribute,\n",
        "    #calc the ig, n return q. that produces the highest gain\n",
        "    gain, ques = find_best_split(rows)\n",
        "    \n",
        "    #base case: no further info gain, since we can ask no further q. well return a leaf\n",
        "    if gain==0:\n",
        "        return leaf(rows)\n",
        "    \n",
        "    #if we reach here we have found a useful feature/ value to partition on\n",
        "    true_rows, false_rows = partition(rows, ques)\n",
        "    \n",
        "    #recursively build the true branch\n",
        "    true_branch = build_tree(true_rows)\n",
        "    \n",
        "    #return a q. node\n",
        "    #this records the best feature/ value to ask at this pt. \n",
        "    #as well branches to follow depending on the ans.\n",
        "    return decision_node(ques, true_branch, false_branch)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgMdweLDJOeE"
      },
      "source": [
        "def print_tree(node, spacing=\"\"):\n",
        "    '''best elegant tree printing fn'''\n",
        "\n",
        "    #base case: we've reached a leaf\n",
        "    if isinstance(node, leaf):\n",
        "        print(spacing + \"predict \", node.predictions)\n",
        "        return\n",
        "    \n",
        "    #print the q.\n",
        "    print(spacing + str(node.ques))\n",
        "    \n",
        "    #call this fn recursively on the true branch\n",
        "    print(spacing + '-->True:')\n",
        "    print_tree(node.true_branch, spacing+' ')\n",
        "    \n",
        "    ##call this fn recursively on the false branch\n",
        "    print(spacing + '-->True:')\n",
        "    print_tree(node.false_branch, spacing+' ')\n",
        "    \n",
        "def classify(row, node):\n",
        "    \n",
        "    #basecase: we've reached a leaf\n",
        "    if isinstance(node, leaf):\n",
        "        return node.predictions\n",
        "    \n",
        "    #decide whether to follow the true-branch or false-branch\n",
        "    #compare feature/value stored in the node to the eg we're considering\n",
        "    if node.question.match(row):\n",
        "        return classify(row, node.true_branch)\n",
        "    else:\n",
        "        return classify(row, node.false_branch)\n",
        "    \n",
        "#demo:printing that a bit nicer\n",
        "#print_leaf(classify(training_data[0], my_train))\n",
        "\n",
        "if __name__=='__main__':\n",
        "    \n",
        "    my_tree = build_tree(train_data)\n",
        "    \n",
        "    print_tree(my_tree)\n",
        "    \n",
        "    #evaluate\n",
        "    testing_data=[\n",
        "        ['green', 3, 'mango'],\n",
        "    ['yellow', 3, 'mango'],\n",
        "    ['red', 1, 'grape'],\n",
        "    ['red', 1, 'grape'],\n",
        "    ['yellow', 3, 'lemon'],]\n",
        "    \n",
        "    for row in testing_data:\n",
        "        print(\"Actual: %s. Predicted: %s\" %\n",
        "             (row[-1], print_leaf(classify(row, my_tree))))\n",
        "        \n",
        "#for next step add support for missing(or unseen) attribute\n",
        "#prune the tree to prevent overfitting\n",
        "#add supportfor regression\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}